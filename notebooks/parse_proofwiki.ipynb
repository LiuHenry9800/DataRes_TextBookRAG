{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the NaturalProofs ProofWiki domain\n",
    "\n",
    "This notebook is used to create NaturalProofs's ProofWiki domain (`naturalproofs_proofwiki.json`).\n",
    "\n",
    "ProofWiki provides a website dump here:\n",
    "- https://proofwiki.org/wiki/User:Afirou/website_dump\n",
    "\n",
    "Download the ProofWiki XML dump we used [here](https://drive.google.com/file/d/1pg6ae7xt-PO0ot4F_iJv9uhTLJ8Mr6Gi/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4 wikitextparser nltk jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from nltk import ngrams\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import wikitextparser as wtp\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './proof_wiki_nov_12_2020.xml'\n",
    "\n",
    "soup = BS(open(filepath, 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse redirects\n",
    "\n",
    "We do this first so that we can store links using their redirected names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7617/7617 [00:00<00:00, 34631.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7617 redirects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "redirects = {}\n",
    "\n",
    "pages = soup.find_all('page')\n",
    "pages = [page for page in pages if (\n",
    "    (page.redirect is not None) and\n",
    "    (not page.title.text.startswith('Talk:')) and\n",
    "    (not page.title.text.startswith('User:')) and\n",
    "    (not page.title.text.startswith('User talk:')) and\n",
    "    (not page.title.text.startswith('Help:'))\n",
    ")]\n",
    "\n",
    "for page in tqdm(pages, total=len(pages)):\n",
    "    redirects[page.title.text] = page.redirect['title']\n",
    "\n",
    "print(\"%d redirects\" % len(redirects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse theorem title and theorem statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20203/20203 [00:24<00:00, 828.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19734 parsed, 469 exceptions.\n",
      "has theorem content: 16473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "parsed = []\n",
    "name_to_parsed = {}\n",
    "\n",
    "\n",
    "pages = soup.find_all('page')\n",
    "item_pages = [page for page in pages if (\n",
    "    (\"== Theorem ==\" in page.text) and\n",
    "    (not page.title.text.startswith('Talk:')) and\n",
    "    (not page.title.text.startswith('User:')) and\n",
    "    (not page.title.text.startswith('User talk:')) and\n",
    "    (not page.title.text.startswith('Help:')) and\n",
    "    (not page.redirect)\n",
    ")]\n",
    "exceptions = []\n",
    "discarded = []\n",
    "n = 0\n",
    "for page in tqdm(item_pages, total=len(item_pages)):\n",
    "    # title of the theorem\n",
    "    theorem_title = page.title.text.replace('\\u200e', '')\n",
    "\n",
    "    # parse WikiMedia format\n",
    "    text = page.text.replace('\\u200e', '').replace('\\u2062', '')\n",
    "    wnode = wtp.parse(text)\n",
    "    \n",
    "    # get the WikiMedia Section that has \"== Theorem ==\" as its title\n",
    "    theorem_sections = [s for s in wnode.sections if s.title is not None and s.title.strip() == 'Theorem']\n",
    "    \n",
    "    if len(theorem_sections) == 0:\n",
    "        exceptions.append(page)\n",
    "        continue\n",
    "    else:\n",
    "        theorem_section = theorem_sections[0]\n",
    "    \n",
    "    # get the content inside the <onlyinclude> tag, if there is one.\n",
    "    tags = [t for t in theorem_section.tags() if t.name == 'onlyinclude']\n",
    "    if len(tags) == 0:\n",
    "        # remove subsections \n",
    "        contents = theorem_section.contents.strip().split('\\n\\n===')[0].strip()\n",
    "        plain_text = theorem_section.plain_text().split(\"\\n\\n===\")[0].strip()\n",
    "        links = [l for l in theorem_section.wikilinks if l.title in contents]\n",
    "    else:\n",
    "        n += 1\n",
    "        contents = tags[0].contents.strip()\n",
    "        plain_text = tags[0].plain_text().strip()\n",
    "        links = tags[0].wikilinks\n",
    "        \n",
    "    if plain_text == '== Theorem ==':\n",
    "        contents = \"\"\n",
    "        links = []\n",
    "\n",
    "    links = [l.title for l in links]\n",
    "    links = [redirects.get(l, l) for l in links]\n",
    "    \n",
    "    categories = [node.title.split('Category:')[1] for node in wnode.wikilinks if node.title.startswith('Category:')]\n",
    "    \n",
    "    data = {\n",
    "        'type': 'theorem',\n",
    "        'title': theorem_title,\n",
    "        'contents': contents,\n",
    "        'full_contents': text,\n",
    "        'links': links,\n",
    "        'has_contents': contents != '',\n",
    "        'categories': categories,\n",
    "    }\n",
    "    parsed.append(data)\n",
    "    name_to_parsed[theorem_title] = data\n",
    "    \n",
    "print(\"%d parsed, %d exceptions.\" % (len(parsed), len(exceptions)))\n",
    "print(\"has theorem content: %d\" % (len([x for x in parsed if x['has_contents']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12420/12420 [00:07<00:00, 1576.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12420 parsed\n",
      "has definition content: 9982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_def = []\n",
    "name_to_parsed_def = {}\n",
    "\n",
    "\n",
    "pages = soup.find_all('page')\n",
    "item_pages = [page for page in pages if (\n",
    "    (\"Definition:\" in page.title.text) and\n",
    "    (not page.title.text.startswith('Talk:')) and\n",
    "    (not page.title.text.startswith('User:')) and\n",
    "    (not page.title.text.startswith('User talk:')) and\n",
    "    (not page.title.text.startswith('Help:')) and \n",
    "    (not page.redirect) and\n",
    "    (not page.title.text in name_to_parsed)  # some 'definitions' are actually theorem pages, e.g. Definition:Stabilizer\n",
    ")]\n",
    "discarded = []\n",
    "for page in tqdm(item_pages, total=len(item_pages)):\n",
    "    title = page.title.text.replace('\\u200e', '').replace('\\u2062', '')\n",
    "    \n",
    "    # parse WikiMedia format\n",
    "    text = page.text.replace('\\u200e', '').replace('\\u2062', '')\n",
    "    wnode = wtp.parse(text)\n",
    "    \n",
    "    sections = [s for s in wnode.sections if s.title is not None and s.title.strip() == 'Definition']\n",
    "\n",
    "    if len(sections) == 0:\n",
    "        contents = \"\"\n",
    "        links = []\n",
    "    else:\n",
    "        section = sections[0]\n",
    "\n",
    "        # get the content inside the <onlyinclude> tag, if there is one.\n",
    "        tags = [t for t in section.tags() if t.name == 'onlyinclude']\n",
    "        if len(tags) == 0:\n",
    "            contents = section.contents.strip()\n",
    "            links = section.wikilinks\n",
    "            links = [l.title for l in links]\n",
    "        else:\n",
    "            contents = tags[0].contents.strip()\n",
    "            links = tags[0].wikilinks\n",
    "            links = [l.title for l in links]\n",
    "    \n",
    "    links = [redirects.get(l, l) for l in links]\n",
    "    categories = [node.title.split('Category:')[1] for node in wnode.wikilinks if node.title.startswith('Category:')]\n",
    "   \n",
    "    data = {\n",
    "        'type': 'definition',\n",
    "        'title': title,\n",
    "        'contents': contents,\n",
    "        'full_contents': text,\n",
    "        'links': links,\n",
    "        'has_contents': contents != \"\",\n",
    "        'categories': categories\n",
    "    }\n",
    "    parsed_def.append(data)\n",
    "    name_to_parsed_def[title] = data\n",
    "    \n",
    "\n",
    "print(\"%d parsed\" % (len(parsed_def)))\n",
    "print(\"has definition content: %d\" % (len([x for x in parsed_def if x['has_contents']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67077/67077 [00:33<00:00, 1973.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4988 2019\n",
      "19956 parsed, 46454 exceptions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_proof = []\n",
    "name_to_parsed_proof = {}\n",
    "\n",
    "pages = soup.find_all('page')\n",
    "item_pages = [page for page in pages if (\n",
    "    (not page.title.text.startswith('Talk:')) and\n",
    "    (not page.title.text.startswith('User:')) and\n",
    "    (not page.title.text.startswith('User talk:')) and\n",
    "    (not page.title.text.startswith('Help:'))\n",
    ")]\n",
    "exceptions = []\n",
    "discarded = []\n",
    "missing_links = []\n",
    "\n",
    "for page in tqdm(item_pages, total=len(item_pages)):\n",
    "    title = page.title.text.strip('\\u200e')\n",
    "        \n",
    "    # parse WikiMedia format\n",
    "    text = page.text.replace('\\u200e', '').replace('\\u2062', '')\n",
    "    wnode = wtp.parse(text)  \n",
    "    \n",
    "    # get the WikiMedia Section that has \"== Proof ==\" as its title\n",
    "    sections = [s for s in wnode.sections if s.title is not None and s.title.strip() == 'Proof']\n",
    "\n",
    "    if len(sections) == 0:\n",
    "        exceptions.append(page)\n",
    "        continue\n",
    "    else:\n",
    "        section = sections[0]\n",
    "\n",
    "    # get the content inside the <onlyinclude> tag, if there is one.\n",
    "    tags = [t for t in section.tags() if t.name == 'onlyinclude']\n",
    "    if len(tags) == 0:\n",
    "        contents = section.contents.strip()\n",
    "        links = section.wikilinks\n",
    "    else:\n",
    "        contents = tags[0].contents.strip()\n",
    "        links = tags[0].wikilinks\n",
    "\n",
    "    links = [l.title.strip('\\u200e') for l in links]\n",
    "    links = [redirects.get(l, l) for l in links]\n",
    "\n",
    "        \n",
    "    for ltitle in links:\n",
    "        if ltitle not in name_to_parsed and ltitle not in name_to_parsed_def:\n",
    "            if ltitle.startswith('Category:') or ltitle.startswith('File:'):\n",
    "                continue\n",
    "            missing_links.append(ltitle)\n",
    "    \n",
    "    # skip proofs without a proof\n",
    "    if contents == '{{ProofWanted}}' or contents == '{{proof wanted}}' or contents == '{{finish}}' or contents == '{{Finish}}':\n",
    "        continue\n",
    "        \n",
    "    categories = [node.title.split('Category:')[1] for node in wnode.wikilinks if node.title.startswith('Category:')]\n",
    "\n",
    "    data = {\n",
    "        'type': 'proof',\n",
    "        'title': title,\n",
    "        'contents': contents,\n",
    "        'links': links,\n",
    "        'categories': categories\n",
    "    }\n",
    "    parsed_proof.append(data)\n",
    "    name_to_parsed_proof[title] = data\n",
    "    \n",
    "\n",
    "print(len(missing_links), len(set(missing_links)))\n",
    "print(\"%d parsed, %d exceptions.\" % (len(parsed_proof), len(exceptions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse pages that are linked to that we missed\n",
    "\n",
    "- Axioms\n",
    "- Proof techniques\n",
    "- Corollaries\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1006/1006 [00:00<00:00, 2127.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006 parsed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "missing_links_set = set(missing_links)\n",
    "pages = soup.find_all('page')\n",
    "pages = [page for page in pages if page.title.text in missing_links_set]\n",
    "\n",
    "parsed_extra = []\n",
    "name_to_parsed_extra = {}\n",
    "\n",
    "for page in tqdm(pages, total=len(pages)):\n",
    "    # title of the theorem\n",
    "    title = page.title.text.strip('\\u200e')\n",
    "    \n",
    "    # parse WikiMedia format\n",
    "    text = page.text.replace('\\u200e', '').replace('\\u2062', '')\n",
    "    wnode = wtp.parse(text)  \n",
    "    \n",
    "    links = wnode.wikilinks if wnode.wikilinks is not None else []\n",
    "    links = [l.title.strip('\\u200e') for l in links]\n",
    "    links = [redirects.get(l, l) for l in links]\n",
    "\n",
    "    categories = [node.title.split('Category:')[1] for node in wnode.wikilinks if node.title.startswith('Category:')]\n",
    "\n",
    "    data = {\n",
    "        'type': 'extra',\n",
    "        'title': title,\n",
    "        'contents': text,\n",
    "        'full_contents': text,\n",
    "        'has_contents': True,\n",
    "        'links': links,\n",
    "        'categories': categories\n",
    "    }\n",
    "    parsed_extra.append(data)\n",
    "    name_to_parsed_extra[title] = data\n",
    "    \n",
    "\n",
    "print(\"%d parsed.\" % (len(parsed_extra)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we remove the remaining missing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33160\n"
     ]
    }
   ],
   "source": [
    "link_set = set()\n",
    "for name in name_to_parsed:\n",
    "    link_set.add(name)\n",
    "\n",
    "for name in name_to_parsed_def:\n",
    "    link_set.add(name)\n",
    "    \n",
    "for name in name_to_parsed_extra:\n",
    "    link_set.add(name)\n",
    "    \n",
    "print(len(link_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, item in name_to_parsed.items():\n",
    "    links = item['links']\n",
    "    links = [l for l in links if l in link_set]\n",
    "    item['links'] = links\n",
    "\n",
    "for name, item in name_to_parsed_def.items():\n",
    "    links = item['links']\n",
    "    links = [l for l in links if l in link_set]\n",
    "    item['links'] = links\n",
    "    \n",
    "for name, item in name_to_parsed_proof.items():\n",
    "    links = item['links']\n",
    "    links = [l for l in links if l in link_set]\n",
    "    item['links'] = links\n",
    "    \n",
    "for name, item in name_to_parsed_extra.items():\n",
    "    links = item['links']\n",
    "    links = [l for l in links if l in link_set]\n",
    "    item['links'] = links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a flag to denote whether a theorem has a proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19734/19734 [01:22<00:00, 239.83it/s]\n"
     ]
    }
   ],
   "source": [
    "theorem_no_proof = set()\n",
    "\n",
    "for name in tqdm(name_to_parsed, total=len(name_to_parsed)):\n",
    "    theorem = name_to_parsed[name]\n",
    "    proof_names = []\n",
    "    for proof_name in name_to_parsed_proof:\n",
    "        if name == proof_name:\n",
    "            proof_names.append(proof_name)\n",
    "        elif name in proof_name and '/' in proof_name and len(proof_name.split('/')) == 2:\n",
    "            suffix = proof_name.split('/')[-1]\n",
    "            if 'proof' in suffix.lower():\n",
    "                proof_names.append(proof_name)\n",
    "        else:  # NOTE: we discard some proofs this way\n",
    "            pass\n",
    "    theorem['proof_names'] = proof_names\n",
    "    theorem['has_proof'] = len(proof_names) > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_links(lines):\n",
    "    def __replace(line):\n",
    "        matches = re.findall(r'(\\[\\[([^]]*)\\]\\])', line)\n",
    "        for match in matches:\n",
    "            full, inner = match\n",
    "            splt = inner.split('|')\n",
    "            if len(splt) == 1:\n",
    "                txt = splt[0]\n",
    "            elif len(splt) == 2:\n",
    "                txt = splt[1]\n",
    "            else:\n",
    "                txt = ''.join(splt[1:])\n",
    "            if full in line:\n",
    "                line = line.replace(full, txt)\n",
    "        return line\n",
    "    lines_ = [\n",
    "        __replace(line) for line in lines\n",
    "    ]\n",
    "    return lines_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorems with proofs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_json = {\n",
    "    'examples': [],\n",
    "    'theorems': [],\n",
    "    'definitions': [],\n",
    "    'other': [],\n",
    "    'proofs': []\n",
    "}\n",
    "\n",
    "# `examples`: contains theorems that have contents and at least one proof with at least one reference \n",
    "for theorem_name in name_to_parsed:\n",
    "    theorem = name_to_parsed[theorem_name]\n",
    "    \n",
    "    if not theorem['has_proof'] or not theorem['has_contents']:\n",
    "        continue\n",
    "    \n",
    "    example = {}\n",
    "    example['type'] = 'theorem'\n",
    "    example['has_proof'] = theorem['has_proof']\n",
    "    example['title'] = theorem['title']\n",
    "    example['proof_titles'] = theorem['proof_names']\n",
    "    example['categories'] = theorem['categories']\n",
    "    example['statement'] = {\n",
    "        'contents': [line for line in theorem['contents'].split('\\n') if line != ''],\n",
    "        'refs': theorem['links'],\n",
    "    }\n",
    "    example['statement']['read_contents'] = replace_links(example['statement']['contents'])\n",
    "\n",
    "    nrefs = 0\n",
    "    example['proofs'] = []    \n",
    "    for proof_name in theorem['proof_names']:\n",
    "        proof_ = name_to_parsed_proof[proof_name]\n",
    "        proof = {\n",
    "            'title': proof_name,\n",
    "            'refs': name_to_parsed_proof[proof_name]['links']\n",
    "        }\n",
    "        example['proofs'].append(proof)\n",
    "        \n",
    "        nrefs += len(proof['refs'])\n",
    "    \n",
    "    # only keep if there is at least one reference\n",
    "    if nrefs == 0:\n",
    "        continue\n",
    "    \n",
    "    if len(example['proofs']) == 0:\n",
    "        print(theorem_name)\n",
    "        \n",
    "    examples_json['examples'].append(example)\n",
    "\n",
    "    \n",
    "# store _all_ theorems (including `full_contents` as well) separately\n",
    "for name in name_to_parsed:\n",
    "    item = name_to_parsed[name]\n",
    "    example = {}\n",
    "    example['type'] = 'theorem'\n",
    "    example['has_proof'] = item['has_proof']\n",
    "    example['has_contents'] = item['has_contents']\n",
    "    example['title'] = item['title']\n",
    "    example['proof_titles'] = item['proof_names']\n",
    "    example['contents'] = [line for line in item['contents'].split('\\n') if line != '']\n",
    "    example['read_contents'] = replace_links(example['contents'])\n",
    "    example['refs'] = item['links']\n",
    "    example['categories'] = item['categories']\n",
    "    examples_json['theorems'].append(example)\n",
    "    \n",
    "# store all proofs separately\n",
    "for name in name_to_parsed_proof:\n",
    "    item = name_to_parsed_proof[name]\n",
    "    example = {\n",
    "        'type': 'proof',\n",
    "        'title': item['title'],\n",
    "        'contents': [line for line in item['contents'].split('\\n') if line != ''],\n",
    "        'refs': item['links'],\n",
    "        'categories': item['categories']\n",
    "    }\n",
    "    example['read_contents'] = replace_links(example['contents'])\n",
    "    examples_json['proofs'].append(example)\n",
    "\n",
    "# store all definitions separately\n",
    "for name in name_to_parsed_def:\n",
    "    item = name_to_parsed_def[name]\n",
    "    example = {\n",
    "        'type': 'definition',\n",
    "        'title': item['title'],\n",
    "        'has_contents': item['has_contents'],\n",
    "        'contents': [line for line in item['contents'].split('\\n') if line != ''],\n",
    "        'refs': item['links'],\n",
    "        'categories': item['categories']\n",
    "    }\n",
    "    example['read_contents'] = replace_links(example['contents'])\n",
    "    examples_json['definitions'].append(example)\n",
    "\n",
    "# store all additional pages that are linked to\n",
    "for name in name_to_parsed_extra:\n",
    "    item = name_to_parsed_extra[name]\n",
    "    example = {\n",
    "        'type': 'other',\n",
    "        'title': item['title'],\n",
    "        'has_contents': item['has_contents'],\n",
    "        'contents': [line for line in item['contents'].split('\\n') if line != ''],\n",
    "        'refs': item['links'],\n",
    "        'categories': item['categories']\n",
    "    }\n",
    "    example['read_contents'] = replace_links(example['contents'])\n",
    "    examples_json['other'].append(example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicate examples\n",
    "- $(x_1,y_1),(x_2,y_2)$ such that the contents of $y_1$ exactly matches the contents of $y_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13859/13859 [05:17<00:00, 43.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "duplicates = defaultdict(list)\n",
    "for example in tqdm(examples_json['examples']):\n",
    "    for example2 in examples_json['examples']:\n",
    "        if example['title'] == example2['title']:\n",
    "            continue\n",
    "        \n",
    "        for proof1_title in example['proof_titles']:\n",
    "            for proof2_title in example2['proof_titles']:\n",
    "                proof1 = name_to_parsed_proof[proof1_title]\n",
    "                proof2 = name_to_parsed_proof[proof2_title]\n",
    "                if proof1['contents'] == proof2['contents']:\n",
    "                    duplicates[example['title']].append(example2['title'])\n",
    "                    \n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed = set()\n",
    "to_remove = []\n",
    "for title in sorted(duplicates.keys()):\n",
    "    if title in removed:\n",
    "        continue\n",
    "    \n",
    "    for dup_title in duplicates[title]:\n",
    "        if dup_title not in removed:\n",
    "            to_remove.append(dup_title)\n",
    "            removed.add(dup_title)\n",
    "\n",
    "len(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [x for x in examples_json['examples'] if x['title'] not in to_remove]\n",
    "examples_json['examples'] = examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorem statement is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13660/13660 [02:57<00:00, 76.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "duplicates = defaultdict(list)\n",
    "for example in tqdm(examples_json['examples']):\n",
    "    for example2 in examples_json['examples']:\n",
    "        if example['title'] == example2['title']:\n",
    "            continue\n",
    "        \n",
    "        if ''.join(example['statement']['contents']) == ''.join(example2['statement']['contents']):\n",
    "            duplicates[example['title']].append(example2['title'])\n",
    "                    \n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed = set()\n",
    "to_remove = []\n",
    "for title in sorted(duplicates.keys()):\n",
    "    if title in removed:\n",
    "        continue\n",
    "    \n",
    "    for dup_title in duplicates[title]:\n",
    "        if dup_title not in removed:\n",
    "            to_remove.append(dup_title)\n",
    "            removed.add(dup_title)\n",
    "\n",
    "len(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [x for x in examples_json['examples'] if x['title'] not in to_remove]\n",
    "examples_json['examples'] = examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, assign each reference (theorem/definition/other) a unique id, and include the reference ids in the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_id = {}\n",
    "proof_name_to_id = {}\n",
    "retrieval_set = []\n",
    "for item in examples_json['theorems']:\n",
    "    if item['title'] not in name_to_id:\n",
    "        name_to_id[item['title']] = len(name_to_id)\n",
    "        item['id'] = name_to_id[item['title']]\n",
    "    else:\n",
    "        print(item['title'])\n",
    "        \n",
    "for item in examples_json['definitions']:\n",
    "    if item['title'] not in name_to_id:\n",
    "        name_to_id[item['title']] = len(name_to_id)\n",
    "        item['id'] = name_to_id[item['title']]\n",
    "    else:\n",
    "        print(item['title'])\n",
    "        \n",
    "for item in examples_json['other']:\n",
    "    if item['title'] not in name_to_id:\n",
    "        name_to_id[item['title']] = len(name_to_id)\n",
    "        item['id'] = name_to_id[item['title']]\n",
    "    else:\n",
    "        print(item['title'])\n",
    "\n",
    "for i, item in enumerate(examples_json['proofs']):\n",
    "    name = 'proof_'+item['title']\n",
    "    if name not in name_to_id:\n",
    "        proof_name_to_id[name] = i\n",
    "        item['proof_id'] = proof_name_to_id[name]\n",
    "    else:\n",
    "        print(name)\n",
    "        \n",
    "for i, example in enumerate(examples_json['examples']):\n",
    "    example['example_id'] = i\n",
    "    example['theorem_id'] = name_to_id[example['title']]\n",
    "    # references in statement\n",
    "    ref_ids = [name_to_id[ref] for ref in example['statement']['refs']]\n",
    "    example['statement']['ref_ids'] = ref_ids\n",
    "    \n",
    "    # references in proofs\n",
    "    for proof in example['proofs']:\n",
    "        ref_ids = [name_to_id[ref] for ref in proof['refs']]\n",
    "        proof['ref_ids'] = ref_ids\n",
    "        proof['proof_id'] = proof_name_to_id['proof_'+proof['title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename `examples` as `retrieval_examples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_json['retrieval_examples'] = examples_json.pop('examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theorems\t19734\n",
      "definitions\t12420\n",
      "other\t1006\n",
      "proofs\t19956\n",
      "retrieval_examples\t13597\n"
     ]
    }
   ],
   "source": [
    "for k, vs in examples_json.items():\n",
    "    print(\"%s\\t%d\" % (k, len(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'dataset': examples_json,\n",
    "}\n",
    "\n",
    "\n",
    "import json\n",
    "output_json = './naturalproofs_proofwiki.json'\n",
    "with open(output_json, 'w') as f:\n",
    "    json.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset splits\n",
    "\n",
    "Form dataset splits using the reference graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint as pp\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = json.load(open(output_json, 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form the reference graph\n",
    "\n",
    "$G=(V,E)$\n",
    "- $v\\in V$: theorem, definition, other page\n",
    "- $(u, v)\\in E$: $u$ occurs in the statement or a proof of $v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "877 1-cycles\n"
     ]
    }
   ],
   "source": [
    "refs = raw_ds['dataset']['theorems'] + raw_ds['dataset']['definitions'] + raw_ds['dataset']['other']\n",
    "\n",
    "graph = defaultdict(list)\n",
    "\n",
    "id2ref = {}\n",
    "ref2id = {}\n",
    "for r in refs:\n",
    "    ref2id[r['title']] = r['id']\n",
    "    id2ref[r['id']] = r\n",
    "    \n",
    "title2proof = {}\n",
    "for p in raw_ds['dataset']['proofs']:\n",
    "    title2proof[p['title']] = p\n",
    "    \n",
    "pairs = []\n",
    "cycles = []\n",
    "for r1 in refs:\n",
    "    \n",
    "    # Make an edge for each reference in the _statement_\n",
    "    for r2 in r1['refs']:\n",
    "        \n",
    "        r1id = r1['id']\n",
    "        r2id = ref2id[r2]\n",
    "        \n",
    "        if r1id != r2id:\n",
    "            graph[r2id].append(r1id)\n",
    "            \n",
    "            pairs.append((r2id, r1id))\n",
    "            \n",
    "            if r2id in graph[r1id]:\n",
    "                cycles.append(tuple(sorted((r2id, r1id))))\n",
    "                \n",
    "    # Make an edge for each reference in the _proof_ (when available)\n",
    "    if r1['type'] == 'theorem' and r1['has_proof']:\n",
    "        for title in r1['proof_titles']:\n",
    "            proof = title2proof[title]\n",
    "            \n",
    "            for r2 in proof['refs']:                \n",
    "                r1id = r1['id']\n",
    "                r2id = ref2id[r2]\n",
    "                if r1id != r2id:\n",
    "                    graph[r2id].append(r1id)\n",
    "                    \n",
    "                    pairs.append((r2id, r1id))\n",
    "\n",
    "                    if r2id in graph[r1id]:\n",
    "                        cycles.append(tuple(sorted((r2id, r1id))))\n",
    "\n",
    "cycles = set(cycles)\n",
    "print(\"%d 1-cycles\" % (len(cycles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30681 nodes\n",
      "12396 leaf\n",
      "18285 non-leaf\n",
      "\n",
      "1392 heads\n"
     ]
    }
   ],
   "source": [
    "G = networkx.DiGraph(graph)\n",
    "leafs = [node for node in G.nodes() if G.in_degree(node) != 0 and G.out_degree(node)==0]\n",
    "nonleafs = [node for node in G.nodes() if G.in_degree(node) == 0 or G.out_degree(node) != 0]\n",
    "heads = [node for node in G.nodes() if G.in_degree(node) == 0 and G.out_degree(node) > 0]\n",
    "\n",
    "print(\"%d nodes\\n%d leaf\\n%d non-leaf\\n\\n%d heads\" % (\n",
    "    len(G.nodes()),\n",
    "    len(leafs),\n",
    "    len(nonleafs),\n",
    "    len(heads)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BFS layers\n",
    "\n",
    "Form BFS layers, count the number of nodes, example-worthy theorems (has proof(s) + contents), and 1-cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total nodes 30681\n",
      "\n",
      "layer\tnodes\tthms\tleaf_thms\n",
      "0\t1392\t0\t0\n",
      "1\t10850\t5777\t2376\n",
      "2\t11713\t5644\t2581\n",
      "3\t5239\t1898\t788\n",
      "4\t1322\t252\t107\n",
      "5\t141\t25\t12\n",
      "6\t14\t1\t0\n"
     ]
    }
   ],
   "source": [
    "print(\"total nodes %d\\n\" % len(G.nodes()))\n",
    "\n",
    "incycle = set()\n",
    "for a, b in cycles:\n",
    "    incycle.add(a)\n",
    "    incycle.add(b)\n",
    "\n",
    "# theorems that correspond to examples (e.g. has a proof, contents)\n",
    "tid2eid = {}\n",
    "for item in raw_ds['dataset']['retrieval_examples']:\n",
    "    tid2eid[item['theorem_id']] = item['example_id']\n",
    "    \n",
    "layers = defaultdict(set)\n",
    "nleafs = []\n",
    "\n",
    "seen = set()\n",
    "for node in heads:\n",
    "    layers[0].add(node)\n",
    "    seen.add(node)\n",
    "    \n",
    "layer = 0\n",
    "print('layer', 'nodes', 'thms', 'leaf_thms', sep='\\t')\n",
    "\n",
    "while len(layers[layer]) > 0:\n",
    "    thms = [x for x in layers[layer] if x in tid2eid]\n",
    "    leaf_thms = [x for x in layers[layer] if x in tid2eid\n",
    "        and x in leafs\n",
    "        and (x not in incycle)\n",
    "    ]\n",
    "    nleafs.append(len(leaf_thms))\n",
    "    \n",
    "    print(layer, len(layers[layer]), len(thms), len(leaf_thms), sep='\\t')\n",
    "    for node in layers[layer]:\n",
    "        for child in G.successors(node):\n",
    "            if child not in seen:\n",
    "                layers[layer+1].add(child)\n",
    "                seen.add(child)\n",
    "    layer += 1\n",
    "\n",
    "nleafs.append(0)\n",
    "    \n",
    "nleafs = np.array(nleafs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the train, valid, test splits\n",
    "\n",
    "We define valid $\\cup$ test as leaves, selected at each layer proportional to the number of leaves at the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 2200\n",
    "leaf_frac = nleafs/nleafs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_refs 28473\n",
      "eval_thms 2198\n",
      "eval_refs 2198\n",
      "train_thms 11399\n"
     ]
    }
   ],
   "source": [
    "rand = np.random.RandomState(42)\n",
    "\n",
    "splits = defaultdict(set)\n",
    "\n",
    "for layer in range(len(layers)):\n",
    "        \n",
    "    # get number of eval leaves for this layer\n",
    "    nleaf = int(budget*leaf_frac[layer])\n",
    "    \n",
    "    # randomly sample `nleaf` leaf theorems\n",
    "    leaf_thms = [x for x in layers[layer] if x in tid2eid\n",
    "        and x in leafs\n",
    "        and (x not in incycle)\n",
    "    ]\n",
    "    perm = rand.permutation(len(leaf_thms))\n",
    "    eval_thms = [leaf_thms[i] for i in perm[:nleaf]]\n",
    "    \n",
    "    # collect as evaluation theorems and references\n",
    "    for x in eval_thms:\n",
    "        splits['eval_thms'].add(x)\n",
    "        splits['eval_refs'].add(x)\n",
    "    \n",
    "    # collect all other items as training data\n",
    "    eval_thms_set = set(eval_thms)\n",
    "    for x in layers[layer]:\n",
    "        if x not in eval_thms_set:\n",
    "            splits['train_refs'].add(x)\n",
    "            if x in tid2eid:\n",
    "                splits['train_thms'].add(x)\n",
    "                \n",
    "for k in splits:\n",
    "    splits[k] = list(splits[k])\n",
    "    print(k, len(splits[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify that evaluation theorems are not referred in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2198/2198 [00:42<00:00, 51.90it/s]\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm(splits['eval_thms'], total=len(splits['eval_thms'])):\n",
    "    for y in splits['train_refs']:\n",
    "        if G.has_predecessor(y, x):\n",
    "            print(id2ref[x]['title'], id2ref[y]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly split evaluation into validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = np.random.RandomState(42)\n",
    "perm = rand.permutation(len(splits['eval_thms']))\n",
    "\n",
    "idx = len(splits['eval_thms'])//2\n",
    "val_idxs = perm[:idx]\n",
    "\n",
    "val_thms = [splits['eval_thms'][i] for i in perm[:idx]]\n",
    "tst_thms = [splits['eval_thms'][i] for i in perm[idx:]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert theorem ids to example ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid2eid = {}\n",
    "for item in raw_ds['dataset']['retrieval_examples']:\n",
    "    tid2eid[item['theorem_id']] = item['example_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_splits = {\n",
    "    'train': {},\n",
    "    'valid': {},\n",
    "    'test': {}\n",
    "}\n",
    "\n",
    "final_splits['train']['ref_ids'] = splits['train_refs']\n",
    "final_splits['train']['example_ids'] = [tid2eid[t] for t in splits['train_thms']]\n",
    "\n",
    "final_splits['valid']['ref_ids'] = splits['train_refs'] + splits['eval_refs']\n",
    "final_splits['valid']['example_ids'] = [tid2eid[t] for t in val_thms]\n",
    "\n",
    "final_splits['test']['ref_ids'] = splits['train_refs'] + splits['eval_refs']\n",
    "final_splits['test']['example_ids'] = [tid2eid[t] for t in tst_thms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "ref_ids 28473\n",
      "example_ids 11399\n",
      "\n",
      "valid\n",
      "ref_ids 30671\n",
      "example_ids 1099\n",
      "\n",
      "test\n",
      "ref_ids 30671\n",
      "example_ids 1099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in final_splits:\n",
    "    print(k)\n",
    "    for k2 in final_splits[k]:\n",
    "        print(k2, len(final_splits[k][k2]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds['splits'] = final_splits\n",
    "\n",
    "import json\n",
    "with open(output_json, 'w') as f:\n",
    "    json.dump(raw_ds, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize to NaturalProofs data schema\n",
    "\n",
    "This step converts the data to adhere to the NaturalProofs data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(output_json) as f:\n",
    "    js = json.load(f)\n",
    "\n",
    "print(js.keys())\n",
    "print(js['dataset'].keys())\n",
    "\n",
    "theorems = js['dataset']['theorems']\n",
    "definitions = js['dataset']['definitions']\n",
    "others = js['dataset']['other']\n",
    "proofs = js['dataset']['proofs']\n",
    "retrieval_examples = js['dataset']['retrieval_examples']\n",
    "splits = js['splits']\n",
    "\n",
    "new_theorems = []\n",
    "new_definitions = []\n",
    "new_others = []\n",
    "new_retrieval_examples = []\n",
    "new_splits = {}\n",
    "\n",
    "title2id = {}\n",
    "for item in theorems + definitions + others:\n",
    "    title2id[item['title']] = item['id']\n",
    "\n",
    "print(proofs[0].keys())\n",
    "title2proof = {}\n",
    "for proof in proofs:\n",
    "    title2proof[proof['title']] = {\n",
    "        'contents': proof['contents'],\n",
    "        'refs': proof['refs'],\n",
    "        'ref_ids': [title2id[title] for title in proof['refs']],\n",
    "    }\n",
    "\n",
    "print(theorems[0].keys())\n",
    "for item in theorems:\n",
    "    new_theorems.append({\n",
    "        'id': item['id'],\n",
    "        'type': item['type'],\n",
    "        'label': item['title'],\n",
    "        'title': item['title'],\n",
    "        'categories': item['categories'],\n",
    "        'contents': item['contents'],\n",
    "        'refs': item['refs'],\n",
    "        'ref_ids': [title2id[title] for title in item['refs']],\n",
    "        'proofs': [title2proof[t] for t in item['proof_titles']],\n",
    "    })\n",
    "\n",
    "for item in definitions:\n",
    "    new_definitions.append({\n",
    "        'id': item['id'],\n",
    "        'type': item['type'],\n",
    "        'label': item['title'],\n",
    "        'title': item['title'],\n",
    "        'categories': item['categories'],\n",
    "        'contents': item['contents'],\n",
    "        'refs': item['refs'],\n",
    "        'ref_ids': [title2id[title] for title in item['refs']],\n",
    "        'proofs': [],\n",
    "    })\n",
    "\n",
    "for item in others:\n",
    "    new_others.append({\n",
    "        'id': item['id'],\n",
    "        'type': item['type'],\n",
    "        'label': item['title'],\n",
    "        'title': item['title'],\n",
    "        'categories': item['categories'],\n",
    "        'contents': item['contents'],\n",
    "        'refs': item['refs'],\n",
    "        'ref_ids': [title2id[title] for title in item['refs']],\n",
    "        'proofs': [],\n",
    "    })\n",
    "\n",
    "id2item = {}\n",
    "for item in new_theorems + new_definitions + new_others:\n",
    "    id2item[item['id']] = item\n",
    "\n",
    "eid2tid = {}\n",
    "for e in retrieval_examples:\n",
    "    eid2tid[e['example_id']] = e['theorem_id']\n",
    "    \n",
    "print(retrieval_examples[0].keys())\n",
    "new_retrieval_examples = [e['theorem_id'] for e in retrieval_examples]\n",
    "\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    new_splits[split] = {\n",
    "        'ref_ids': splits[split]['ref_ids'],\n",
    "        'examples': sum([[(eid2tid[eid], j) for j in range(len(id2item[eid2tid[eid]]['proofs'])) \\\n",
    "                                            if len(id2item[eid2tid[eid]]['proofs'][j]['refs']) > 0] \\\n",
    "                         for eid in splits[split]['example_ids']], []),\n",
    "    }\n",
    "\n",
    "js = {\n",
    "    'dataset': {\n",
    "        'theorems': new_theorems,\n",
    "        'definitions': new_definitions,\n",
    "        'others': new_others,\n",
    "        'retrieval_examples': new_retrieval_examples,\n",
    "    },\n",
    "    'splits': new_splits,\n",
    "}\n",
    "\n",
    "with open(output_json, 'w') as f:\n",
    "    json.dump(js, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Graph\n",
    "\n",
    "This step adds extra category tags to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "cats = {}\n",
    "\n",
    "prefix = 'https://www.proofwiki.org/'\n",
    "root = '/wiki/Category:Content_Categories'\n",
    "\n",
    "def dfs(title, suffix, parent):\n",
    "    print(str(len(cats)) + ' ' + title)\n",
    "    url = prefix + suffix\n",
    "    page = requests.get(url)\n",
    "    soup = BS(page.content, 'html.parser')\n",
    "    \n",
    "    children = []\n",
    "    children_titles = []\n",
    "    \n",
    "    for item in soup.find_all('div', class_='CategoryTreeItem'):\n",
    "        a = item.find('a')\n",
    "        children.append((a.text, a['href']))\n",
    "        children_titles.append(a.text)\n",
    "    \n",
    "    cat = {\n",
    "        'title': title,\n",
    "        'suffix': suffix,\n",
    "        'parent': parent,\n",
    "        'children': children_titles,\n",
    "    }\n",
    "    cats[title] = cat\n",
    "    \n",
    "    for (child_title, child_suffix) in children:\n",
    "        if child_title not in cats:\n",
    "            dfs(child_title, child_suffix, title)\n",
    "\n",
    "dfs('Content Categories', root, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cats:\n",
    "    cats[cat]['parents'] = []\n",
    "for cat in cats:\n",
    "    for c in cats[cat]['children']:\n",
    "        cats[c]['parents'].append(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat2kind = {}\n",
    "def dfs2(cat, kind):\n",
    "    if cat in cat2kind and cat2kind[cat] != kind:\n",
    "        print(cat)\n",
    "    cat2kind[cat] = kind\n",
    "    visited.add(cat)\n",
    "    for child in cats[cat]['children']:\n",
    "        if child not in visited:\n",
    "            dfs2(child, kind)\n",
    "\n",
    "visited = set()\n",
    "dfs2('Proofs by Topic', 'proof')\n",
    "visited = set()\n",
    "dfs2('Definitions/Branches of Mathematics', 'definition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_json) as f:\n",
    "    js = json.load(f)\n",
    "\n",
    "theorems = js['dataset']['theorems']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5339 cats appear in dataset\n",
      "49 appeared cats missing from cat graph\n"
     ]
    }
   ],
   "source": [
    "categories = set()\n",
    "for item in theorems + definitions + others:\n",
    "    for cat in item['categories']:\n",
    "        categories.add(cat)\n",
    "print('%d cats appear in dataset' % len(categories))\n",
    "\n",
    "missing = 0\n",
    "for cat in categories:\n",
    "    if cat.strip(' ') not in cats:\n",
    "        all_numbers = True\n",
    "        for c in cat:\n",
    "            if not (c == ',' or '0' <= c <= '9'):\n",
    "                all_numbers = False\n",
    "        if all_numbers:\n",
    "            continue\n",
    "        missing += 1\n",
    "print('%d appeared cats missing from cat graph' % missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toplevel(cat, stack=[]):\n",
    "    if cat is None:\n",
    "        return set()\n",
    "    if cat in stack:\n",
    "        return set()\n",
    "    ans = set()\n",
    "    for p in cats[cat]['parents']:\n",
    "        if p == 'Proofs by Topic':\n",
    "            ans.add(cat)\n",
    "        else:\n",
    "            res = get_toplevel(p, stack + [cat])\n",
    "            for k in res:\n",
    "                ans.add(k)\n",
    "    return ans\n",
    "\n",
    "def get_recursive(cat, stack=[]):\n",
    "    if cat is None:\n",
    "        return set()\n",
    "    if cat in stack:\n",
    "        return set()\n",
    "    for p in cats[cat]['parents']:\n",
    "        if p == 'Proofs by Topic':\n",
    "            recs.add(cat)\n",
    "            for k in stack:\n",
    "                recs.add(k)\n",
    "        else:\n",
    "            get_recursive(p, stack + [cat])\n",
    "\n",
    "for item in theorems:\n",
    "    tls = set()\n",
    "    recs = set()\n",
    "    for cat in item['categories']:\n",
    "        if cat not in cats: continue\n",
    "        res = get_toplevel(cat)\n",
    "        for k in res:\n",
    "            tls.add(k)\n",
    "        get_recursive(cat)\n",
    "    item['toplevel_categories'] = list(tls)\n",
    "    item['recursive_categories'] = list(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "js['dataset']['theorems'] = theorems\n",
    "\n",
    "with open(output_json, 'w') as f:\n",
    "    json.dump(js, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
