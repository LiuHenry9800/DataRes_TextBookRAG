{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating `NaturalProofs` from ProofWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-a9de7e586e0e>:18: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from nltk import ngrams\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import wikitextparser as wtp\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/path/to/proof_wiki_nov_12_2020.xml'\n",
    "\n",
    "soup = BS(open(filepath, 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse redirects\n",
    "\n",
    "We do this first so that we can store links using their redirected names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7617/7617 [00:00<00:00, 20597.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7617 redirects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "redirects = {}\n",
    "\n",
    "pages = soup.find_all('page')\n",
    "pages = [page for page in pages if (\n",
    "    (page.redirect is not None) and\n",
    "    (not page.title.text.startswith('Talk:')) and\n",
    "    (not page.title.text.startswith('User:')) and\n",
    "    (not page.title.text.startswith('User talk:')) and\n",
    "    (not page.title.text.startswith('Help:'))\n",
    ")]\n",
    "\n",
    "for page in tqdm(pages, total=len(pages)):\n",
    "    redirects[page.title.text] = page.redirect['title']\n",
    "\n",
    "print(\"%d redirects\" % len(redirects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse theorem title and theorem statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20203/20203 [00:24<00:00, 828.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19734 parsed, 469 exceptions.\n",
      "has theorem content: 16473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "parsed = []\n",
    "name_to_parsed = {}\n",
    "\n",
    "\n",
    "pages = soup.find_all('page')\n",
    "item_pages = [page for page in pages if (\n",
    "    (\"== Theorem ==\" in page.text) and\n",
    "    (not page.title.text.startswith('Talk:')) and\n",
    "    (not page.title.text.startswith('User:')) and\n",
    "    (not page.title.text.startswith('User talk:')) and\n",
    "    (not page.title.text.startswith('Help:')) and\n",
    "    (not page.redirect)\n",
    ")]\n",
    "exceptions = []\n",
    "discarded = []\n",
    "n = 0\n",
    "for page in tqdm(item_pages, total=len(item_pages)):\n",
    "    # title of the theorem\n",
    "    theorem_title = page.title.text.replace('\\u200e', '')\n",
    "\n",
    "    # parse WikiMedia format\n",
    "    text = page.text.replace('\\u200e', '').replace('\\u2062', '')\n",
    "    wnode = wtp.parse(text)\n",
    "    \n",
    "    # get the WikiMedia Section that has \"== Theorem ==\" as its title\n",
    "    theorem_sections = [s for s in wnode.sections if s.title is not None and s.title.strip() == 'Theorem']\n",
    "    \n",
    "    if len(theorem_sections) == 0:\n",
    "        exceptions.append(page)\n",
    "        continue\n",
    "    else:\n",
    "        theorem_section = theorem_sections[0]\n",
    "    \n",
    "    # get the content inside the <onlyinclude> tag, if there is one.\n",
    "    tags = [t for t in theorem_section.tags() if t.name == 'onlyinclude']\n",
    "    if len(tags) == 0:\n",
    "        # remove subsections \n",
    "        contents = theorem_section.contents.strip().split('\\n\\n===')[0].strip()\n",
    "        plain_text = theorem_section.plain_text().split(\"\\n\\n===\")[0].strip()\n",
    "        links = [l for l in theorem_section.wikilinks if l.title in contents]\n",
    "    else:\n",
    "        n += 1\n",
    "        contents = tags[0].contents.strip()\n",
    "        plain_text = tags[0].plain_text().strip()\n",
    "        links = tags[0].wikilinks\n",
    "        \n",
    "    if plain_text == '== Theorem ==':\n",
    "        contents = \"\"\n",
    "        links = []\n",
    "\n",
    "    links = [l.title for l in links]\n",
    "    links = [redirects.get(l, l) for l in links]\n",
    "    \n",
    "    categories = [node.title.split('Category:')[1] for node in wnode.wikilinks if node.title.startswith('Category:')]\n",
    "    \n",
    "    data = {\n",
    "        'type': 'theorem',\n",
    "        'title': theorem_title,\n",
    "        'contents': contents,\n",
    "        'full_contents': text,\n",
    "        'links': links,\n",
    "        'has_contents': contents != '',\n",
    "        'categories': categories,\n",
    "    }\n",
    "    parsed.append(data)\n",
    "    name_to_parsed[theorem_title] = data\n",
    "    \n",
    "print(\"%d parsed, %d exceptions.\" % (len(parsed), len(exceptions)))\n",
    "print(\"has theorem content: %d\" % (len([x for x in parsed if x['has_contents']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12420/12420 [00:07<00:00, 1576.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12420 parsed\n",
      "has definition content: 9982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_def = []\n",
    "name_to_parsed_def = {}\n",
    "\n",
    "\n",
    "pages = soup.find_all('page')\n",
    "item_pages = [page for page in pages if (\n",
    "    (\"Definition:\" in page.title.text) and\n",
    "    (not page.title.text.startswith('Talk:')) and\n",
    "    (not page.title.text.startswith('User:')) and\n",
    "    (not page.title.text.startswith('User talk:')) and\n",
    "    (not page.title.text.startswith('Help:')) and \n",
    "    (not page.redirect) and\n",
    "    (not page.title.text in name_to_parsed)  # some 'definitions' are actually theorem pages, e.g. Definition:Stabilizer\n",
    ")]\n",
    "discarded = []\n",
    "for page in tqdm(item_pages, total=len(item_pages)):\n",
    "    title = page.title.text.replace('\\u200e', '').replace('\\u2062', '')\n",
    "    \n",
    "    # parse WikiMedia format\n",
    "    text = page.text.replace('\\u200e', '').replace('\\u2062', '')\n",
    "    wnode = wtp.parse(text)\n",
    "    \n",
    "    sections = [s for s in wnode.sections if s.title is not None and s.title.strip() == 'Definition']\n",
    "\n",
    "    if len(sections) == 0:\n",
    "        contents = \"\"\n",
    "        links = []\n",
    "    else:\n",
    "        section = sections[0]\n",
    "\n",
    "        # get the content inside the <onlyinclude> tag, if there is one.\n",
    "        tags = [t for t in section.tags() if t.name == 'onlyinclude']\n",
    "        if len(tags) == 0:\n",
    "            contents = section.contents.strip()\n",
    "            links = section.wikilinks\n",
    "            links = [l.title for l in links]\n",
    "        else:\n",
    "            contents = tags[0].contents.strip()\n",
    "            links = tags[0].wikilinks\n",
    "            links = [l.title for l in links]\n",
    "    \n",
    "    links = [redirects.get(l, l) for l in links]\n",
    "    categories = [node.title.split('Category:')[1] for node in wnode.wikilinks if node.title.startswith('Category:')]\n",
    "   \n",
    "    data = {\n",
    "        'type': 'definition',\n",
    "        'title': title,\n",
    "        'contents': contents,\n",
    "        'full_contents': text,\n",
    "        'links': links,\n",
    "        'has_contents': contents != \"\",\n",
    "        'categories': categories\n",
    "    }\n",
    "    parsed_def.append(data)\n",
    "    name_to_parsed_def[title] = data\n",
    "    \n",
    "\n",
    "print(\"%d parsed\" % (len(parsed_def)))\n",
    "print(\"has definition content: %d\" % (len([x for x in parsed_def if x['has_contents']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67077/67077 [00:33<00:00, 1973.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4988 2019\n",
      "19956 parsed, 46454 exceptions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_proof = []\n",
    "name_to_parsed_proof = {}\n",
    "\n",
    "pages = soup.find_all('page')\n",
    "item_pages = [page for page in pages if (\n",
    "    (not page.title.text.startswith('Talk:')) and\n",
    "    (not page.title.text.startswith('User:')) and\n",
    "    (not page.title.text.startswith('User talk:')) and\n",
    "    (not page.title.text.startswith('Help:'))\n",
    ")]\n",
    "exceptions = []\n",
    "discarded = []\n",
    "missing_links = []\n",
    "\n",
    "for page in tqdm(item_pages, total=len(item_pages)):\n",
    "    title = page.title.text.strip('\\u200e')\n",
    "        \n",
    "    # parse WikiMedia format\n",
    "    text = page.text.replace('\\u200e', '').replace('\\u2062', '')\n",
    "    wnode = wtp.parse(text)  \n",
    "    \n",
    "    # get the WikiMedia Section that has \"== Proof ==\" as its title\n",
    "    sections = [s for s in wnode.sections if s.title is not None and s.title.strip() == 'Proof']\n",
    "\n",
    "    if len(sections) == 0:\n",
    "        exceptions.append(page)\n",
    "        continue\n",
    "    else:\n",
    "        section = sections[0]\n",
    "\n",
    "    # get the content inside the <onlyinclude> tag, if there is one.\n",
    "    tags = [t for t in section.tags() if t.name == 'onlyinclude']\n",
    "    if len(tags) == 0:\n",
    "        contents = section.contents.strip()\n",
    "        links = section.wikilinks\n",
    "    else:\n",
    "        contents = tags[0].contents.strip()\n",
    "        links = tags[0].wikilinks\n",
    "\n",
    "    links = [l.title.strip('\\u200e') for l in links]\n",
    "    links = [redirects.get(l, l) for l in links]\n",
    "     \n",
    "    for ltitle in links:\n",
    "        if ltitle not in name_to_parsed and ltitle not in name_to_parsed_def:\n",
    "            if ltitle.startswith('Category:') or ltitle.startswith('File:'):\n",
    "                continue\n",
    "            missing_links.append(ltitle)\n",
    "    \n",
    "    # skip proofs without a proof\n",
    "    if contents == '{{ProofWanted}}' or contents == '{{proof wanted}}' or contents == '{{finish}}' or contents == '{{Finish}}':\n",
    "        continue\n",
    "        \n",
    "    categories = [node.title.split('Category:')[1] for node in wnode.wikilinks if node.title.startswith('Category:')]\n",
    "\n",
    "    data = {\n",
    "        'type': 'proof',\n",
    "        'title': title,\n",
    "        'contents': contents,\n",
    "        'links': links,\n",
    "        'categories': categories\n",
    "    }\n",
    "    parsed_proof.append(data)\n",
    "    name_to_parsed_proof[title] = data\n",
    "    \n",
    "\n",
    "print(len(missing_links), len(set(missing_links)))\n",
    "print(\"%d parsed, %d exceptions.\" % (len(parsed_proof), len(exceptions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse pages that are linked to that we missed\n",
    "\n",
    "- Axioms\n",
    "- Proof techniques\n",
    "- Corollaries\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1006/1006 [00:00<00:00, 2127.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006 parsed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "missing_links_set = set(missing_links)\n",
    "pages = soup.find_all('page')\n",
    "pages = [page for page in pages if page.title.text in missing_links_set]\n",
    "\n",
    "parsed_extra = []\n",
    "name_to_parsed_extra = {}\n",
    "\n",
    "for page in tqdm(pages, total=len(pages)):\n",
    "    title = page.title.text.strip('\\u200e')\n",
    "    \n",
    "    # parse WikiMedia format\n",
    "    text = page.text.replace('\\u200e', '').replace('\\u2062', '')\n",
    "    wnode = wtp.parse(text)  \n",
    "    \n",
    "    links = wnode.wikilinks if wnode.wikilinks is not None else []\n",
    "    links = [l.title.strip('\\u200e') for l in links]\n",
    "    links = [redirects.get(l, l) for l in links]\n",
    "\n",
    "    categories = [node.title.split('Category:')[1] for node in wnode.wikilinks if node.title.startswith('Category:')]\n",
    "\n",
    "    data = {\n",
    "        'type': 'extra',\n",
    "        'title': title,\n",
    "        'contents': text,\n",
    "        'full_contents': text,\n",
    "        'has_contents': True,\n",
    "        'links': links,\n",
    "        'categories': categories\n",
    "    }\n",
    "    parsed_extra.append(data)\n",
    "    name_to_parsed_extra[title] = data\n",
    "    \n",
    "\n",
    "print(\"%d parsed.\" % (len(parsed_extra)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we remove the remaining missing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33160\n"
     ]
    }
   ],
   "source": [
    "link_set = set()\n",
    "for name in name_to_parsed:\n",
    "    link_set.add(name)\n",
    "\n",
    "for name in name_to_parsed_def:\n",
    "    link_set.add(name)\n",
    "    \n",
    "for name in name_to_parsed_extra:\n",
    "    link_set.add(name)\n",
    "    \n",
    "print(len(link_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, item in name_to_parsed.items():\n",
    "    links = item['links']\n",
    "    links = [l for l in links if l in link_set]\n",
    "    item['links'] = links\n",
    "\n",
    "for name, item in name_to_parsed_def.items():\n",
    "    links = item['links']\n",
    "    links = [l for l in links if l in link_set]\n",
    "    item['links'] = links\n",
    "    \n",
    "for name, item in name_to_parsed_proof.items():\n",
    "    links = item['links']\n",
    "    links = [l for l in links if l in link_set]\n",
    "    item['links'] = links\n",
    "    \n",
    "for name, item in name_to_parsed_extra.items():\n",
    "    links = item['links']\n",
    "    links = [l for l in links if l in link_set]\n",
    "    item['links'] = links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a flag to denote whether a theorem has a proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19734/19734 [01:22<00:00, 239.83it/s]\n"
     ]
    }
   ],
   "source": [
    "theorem_no_proof = set()\n",
    "\n",
    "for name in tqdm(name_to_parsed, total=len(name_to_parsed)):\n",
    "    theorem = name_to_parsed[name]\n",
    "    proof_names = []\n",
    "    for proof_name in name_to_parsed_proof:\n",
    "        if name == proof_name:\n",
    "            proof_names.append(proof_name)\n",
    "        elif name in proof_name and '/' in proof_name and len(proof_name.split('/')) == 2:\n",
    "            suffix = proof_name.split('/')[-1]\n",
    "            if 'proof' in suffix.lower():\n",
    "                proof_names.append(proof_name)\n",
    "        else:  # NOTE: we discard some proofs this way\n",
    "            pass\n",
    "    theorem['proof_names'] = proof_names\n",
    "    theorem['has_proof'] = len(proof_names) > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_links(lines):\n",
    "    def __replace(line):\n",
    "        matches = re.findall(r'(\\[\\[([^]]*)\\]\\])', line)\n",
    "        for match in matches:\n",
    "            full, inner = match\n",
    "            splt = inner.split('|')\n",
    "            if len(splt) == 1:\n",
    "                txt = splt[0]\n",
    "            elif len(splt) == 2:\n",
    "                txt = splt[1]\n",
    "            else:\n",
    "                txt = ''.join(splt[1:])\n",
    "            if full in line:\n",
    "                line = line.replace(full, txt)\n",
    "        return line\n",
    "    lines_ = [\n",
    "        __replace(line) for line in lines\n",
    "    ]\n",
    "    return lines_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorems with proofs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_json = {\n",
    "    'examples': [],\n",
    "    'theorems': [],\n",
    "    'definitions': [],\n",
    "    'other': [],\n",
    "    'proofs': []\n",
    "}\n",
    "\n",
    "# `examples`: contains theorems that have contents and at least one proof with at least one reference \n",
    "for theorem_name in name_to_parsed:\n",
    "    theorem = name_to_parsed[theorem_name]\n",
    "    \n",
    "    if not theorem['has_proof'] or not theorem['has_contents']:\n",
    "        continue\n",
    "    \n",
    "    example = {}\n",
    "    example['type'] = 'theorem'\n",
    "    example['has_proof'] = theorem['has_proof']\n",
    "    example['title'] = theorem['title']\n",
    "    example['proof_titles'] = theorem['proof_names']\n",
    "    example['categories'] = theorem['categories']\n",
    "    example['statement'] = {\n",
    "        'contents': [line for line in theorem['contents'].split('\\n') if line != ''],\n",
    "        'refs': theorem['links'],\n",
    "    }\n",
    "    example['statement']['read_contents'] = replace_links(example['statement']['contents'])\n",
    "\n",
    "    nrefs = 0\n",
    "    example['proofs'] = []    \n",
    "    for proof_name in theorem['proof_names']:\n",
    "        proof_ = name_to_parsed_proof[proof_name]\n",
    "        proof = {\n",
    "            'title': proof_name,\n",
    "            'refs': name_to_parsed_proof[proof_name]['links']\n",
    "        }\n",
    "        example['proofs'].append(proof)\n",
    "        \n",
    "        nrefs += len(proof['refs'])\n",
    "    \n",
    "    # only keep if there is at least one reference\n",
    "    if nrefs == 0:\n",
    "        continue\n",
    "    \n",
    "    if len(example['proofs']) == 0:\n",
    "        print(theorem_name)   \n",
    "    examples_json['examples'].append(example)\n",
    "    \n",
    "# store _all_ theorems (including `full_contents` as well) separately\n",
    "for name in name_to_parsed:\n",
    "    item = name_to_parsed[name]\n",
    "    example = {}\n",
    "    example['type'] = 'theorem'\n",
    "    example['has_proof'] = item['has_proof']\n",
    "    example['has_contents'] = item['has_contents']\n",
    "    example['title'] = item['title']\n",
    "    example['proof_titles'] = item['proof_names']\n",
    "    example['contents'] = [line for line in item['contents'].split('\\n') if line != '']\n",
    "    example['read_contents'] = replace_links(example['contents'])\n",
    "    example['refs'] = item['links']\n",
    "    example['categories'] = item['categories']\n",
    "    examples_json['theorems'].append(example)\n",
    "    \n",
    "# store all proofs separately\n",
    "for name in name_to_parsed_proof:\n",
    "    item = name_to_parsed_proof[name]\n",
    "    example = {\n",
    "        'type': 'proof',\n",
    "        'title': item['title'],\n",
    "        'contents': [line for line in item['contents'].split('\\n') if line != ''],\n",
    "        'refs': item['links'],\n",
    "        'categories': item['categories']\n",
    "    }\n",
    "    example['read_contents'] = replace_links(example['contents'])\n",
    "    examples_json['proofs'].append(example)\n",
    "\n",
    "# store all definitions separately\n",
    "for name in name_to_parsed_def:\n",
    "    item = name_to_parsed_def[name]\n",
    "    example = {\n",
    "        'type': 'definition',\n",
    "        'title': item['title'],\n",
    "        'has_contents': item['has_contents'],\n",
    "        'contents': [line for line in item['contents'].split('\\n') if line != ''],\n",
    "        'refs': item['links'],\n",
    "        'categories': item['categories']\n",
    "    }\n",
    "    example['read_contents'] = replace_links(example['contents'])\n",
    "    examples_json['definitions'].append(example)\n",
    "\n",
    "# store all additional pages that are linked to\n",
    "for name in name_to_parsed_extra:\n",
    "    item = name_to_parsed_extra[name]\n",
    "    example = {\n",
    "        'type': 'other',\n",
    "        'title': item['title'],\n",
    "        'has_contents': item['has_contents'],\n",
    "        'contents': [line for line in item['contents'].split('\\n') if line != ''],\n",
    "        'refs': item['links'],\n",
    "        'categories': item['categories']\n",
    "    }\n",
    "    example['read_contents'] = replace_links(example['contents'])\n",
    "    examples_json['other'].append(example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicate examples\n",
    "- $(x_1,y_1),(x_2,y_2)$ such that the contents of $y_1$ exactly matches the contents of $y_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13859/13859 [05:17<00:00, 43.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "duplicates = defaultdict(list)\n",
    "for example in tqdm(examples_json['examples']):\n",
    "    for example2 in examples_json['examples']:\n",
    "        if example['title'] == example2['title']:\n",
    "            continue\n",
    "        \n",
    "        for proof1_title in example['proof_titles']:\n",
    "            for proof2_title in example2['proof_titles']:\n",
    "                proof1 = name_to_parsed_proof[proof1_title]\n",
    "                proof2 = name_to_parsed_proof[proof2_title]\n",
    "                if proof1['contents'] == proof2['contents']:\n",
    "                    duplicates[example['title']].append(example2['title'])\n",
    "                    \n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed = set()\n",
    "to_remove = []\n",
    "for title in sorted(duplicates.keys()):\n",
    "    if title in removed:\n",
    "        continue\n",
    "    \n",
    "    for dup_title in duplicates[title]:\n",
    "        if dup_title not in removed:\n",
    "            to_remove.append(dup_title)\n",
    "            removed.add(dup_title)\n",
    "\n",
    "len(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [x for x in examples_json['examples'] if x['title'] not in to_remove]\n",
    "examples_json['examples'] = examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "theorem statement is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13660/13660 [02:57<00:00, 76.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "duplicates = defaultdict(list)\n",
    "for example in tqdm(examples_json['examples']):\n",
    "    for example2 in examples_json['examples']:\n",
    "        if example['title'] == example2['title']:\n",
    "            continue\n",
    "        \n",
    "        if ''.join(example['statement']['contents']) == ''.join(example2['statement']['contents']):\n",
    "            duplicates[example['title']].append(example2['title'])\n",
    "                    \n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed = set()\n",
    "to_remove = []\n",
    "for title in sorted(duplicates.keys()):\n",
    "    if title in removed:\n",
    "        continue\n",
    "    \n",
    "    for dup_title in duplicates[title]:\n",
    "        if dup_title not in removed:\n",
    "            to_remove.append(dup_title)\n",
    "            removed.add(dup_title)\n",
    "\n",
    "len(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [x for x in examples_json['examples'] if x['title'] not in to_remove]\n",
    "examples_json['examples'] = examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, assign each reference (theorem/definition/other) a unique id, and include the reference ids in the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_id = {}\n",
    "proof_name_to_id = {}\n",
    "retrieval_set = []\n",
    "for item in examples_json['theorems']:\n",
    "    if item['title'] not in name_to_id:\n",
    "        name_to_id[item['title']] = len(name_to_id)\n",
    "        item['id'] = name_to_id[item['title']]\n",
    "    else:\n",
    "        print(item['title'])\n",
    "        \n",
    "for item in examples_json['definitions']:\n",
    "    if item['title'] not in name_to_id:\n",
    "        name_to_id[item['title']] = len(name_to_id)\n",
    "        item['id'] = name_to_id[item['title']]\n",
    "    else:\n",
    "        print(item['title'])\n",
    "        \n",
    "for item in examples_json['other']:\n",
    "    if item['title'] not in name_to_id:\n",
    "        name_to_id[item['title']] = len(name_to_id)\n",
    "        item['id'] = name_to_id[item['title']]\n",
    "    else:\n",
    "        print(item['title'])\n",
    "\n",
    "for i, item in enumerate(examples_json['proofs']):\n",
    "    name = 'proof_'+item['title']\n",
    "    if name not in name_to_id:\n",
    "        proof_name_to_id[name] = i\n",
    "        item['proof_id'] = proof_name_to_id[name]\n",
    "    else:\n",
    "        print(name)\n",
    "        \n",
    "for i, example in enumerate(examples_json['examples']):\n",
    "    example['example_id'] = i\n",
    "    example['theorem_id'] = name_to_id[example['title']]\n",
    "    # references in statement\n",
    "    ref_ids = [name_to_id[ref] for ref in example['statement']['refs']]\n",
    "    example['statement']['ref_ids'] = ref_ids\n",
    "    \n",
    "    # references in proofs\n",
    "    for proof in example['proofs']:\n",
    "        ref_ids = [name_to_id[ref] for ref in proof['refs']]\n",
    "        proof['ref_ids'] = ref_ids\n",
    "        proof['proof_id'] = proof_name_to_id['proof_'+proof['title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename `examples` as `retrieval_examples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_json['retrieval_examples'] = examples_json.pop('examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'dataset': examples_json,\n",
    "}\n",
    "\n",
    "\n",
    "import json\n",
    "with open('dataset.json', 'w') as f:\n",
    "    json.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
